{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW Algorithm, Retrieval and Evaluation (Recall, precision, AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureVectors import load_images\n",
    "from FeatureVectors import getFeaturesMatrix\n",
    "results, numbers = load_images()\n",
    "allfeatures = getFeaturesMatrix(numbers, results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace every none value by 0 for DTW calculation\n",
    "for key in allfeatures.keys():\n",
    "    length = len(allfeatures[key])\n",
    "    for line in range(0,length):\n",
    "        allfeatures[key][line]=[0.0 if v is None else v for v in allfeatures[key][line]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfeatures['270-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# task folder locations\n",
    "dataset = os.path.join('..', 'dataset')\n",
    "task_fold = os.path.join(dataset, 'task')\n",
    "# training list\n",
    "train_f = open(os.path.join(task_fold, 'train.txt'), \"r\")\n",
    "train_list = []\n",
    "for l in train_f:\n",
    "    train_list.append(l[:3])\n",
    "# validation list\n",
    "val_f = open(os.path.join(task_fold, 'valid.txt'), \"r\")\n",
    "val_list = []\n",
    "for l in val_f:\n",
    "    val_list.append(l[:3])\n",
    "# create training and validation dataset\n",
    "train_dataset = {}\n",
    "val_dataset = {}\n",
    "for key in allfeatures.keys():\n",
    "    if key[:3] in train_list:\n",
    "        train_dataset[key] = allfeatures[key]\n",
    "    elif key[:3] in val_list:\n",
    "        val_dataset[key] = allfeatures[key]\n",
    "    else:\n",
    "        print(f'{key} is not included in training or validation dataset')\n",
    "\n",
    "print('train_dataset and val_dataset created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ground-truth folder location\n",
    "groundtruth = os.path.join(dataset, 'ground-truth')\n",
    "transcription_f = open(os.path.join(groundtruth, 'transcription.txt'), \"r\")\n",
    "transcription = {}\n",
    "for l in transcription_f:\n",
    "    key_word = l.split()\n",
    "    transcription[key_word[0]] = key_word[1]\n",
    "print('Transcriptions loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcription['270-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the words that appear both in training and validation set\n",
    "keywords_f = open(os.path.join(task_fold, 'keywords.txt'), \"r\")\n",
    "keywords_list = []\n",
    "for l in keywords_f:\n",
    "    keywords_list.append(l[:len(l)-1])\n",
    "print('Keywords loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "* [Install tslearn](https://tslearn.readthedocs.io/en/latest/installation.html)\n",
    "* [tslearn.metrics](https://tslearn.readthedocs.io/en/latest/gen_modules/tslearn.metrics.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_results = {}\n",
    "#check if we already calculated the results, if not do it\n",
    "file = os.path.join('cost_results', 'results.pkl')\n",
    "if not os.path.isfile(file):\n",
    "    #warning: the following two for loops takes a lot of time to compute\n",
    "    for val_vector in val_dataset:\n",
    "        # !!! uncomment the following line and add increment such that it works if you want to compute only meaningful results !!!\n",
    "        # if transcription[val_vector] in keywords_list:\n",
    "        cost_results[val_vector] = {}\n",
    "        for train_vector in train_dataset:\n",
    "            cost = dtw(val_dataset[val_vector],\n",
    "                       train_dataset[train_vector],\n",
    "                       global_constraint=\"sakoe_chiba\",\n",
    "                       sakoe_chiba_radius=3)\n",
    "            cost_results[val_vector][train_vector] = cost\n",
    "    #save the cost results\n",
    "    with open('cost_results/results.pkl', 'wb') as f:\n",
    "        pickle.dump(cost_results, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print('DTW results saved')\n",
    "else:\n",
    "    #if the results already exists, load them\n",
    "    with open(file, 'rb') as f:\n",
    "        cost_results = pickle.load(f)\n",
    "    print('DTW results loaded')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_results['300-02-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea :\n",
    "Define thresholds k for considering a certain cost as a limit\n",
    "to choose results below the limit as positive (similar to the test word) \n",
    "and after the limit as negative (not similar to the test word)\n",
    "\n",
    "Then for each k:\n",
    "\n",
    "a)  check from top to k if it is a true or false positive \n",
    "    by comparing the transcription of the test word to the one of the train word\n",
    "    \n",
    "    => compute true positive and false positive\n",
    "    \n",
    "b)  check from k to the end if it is a true or false negative\n",
    "    by comparing the transcription of the test word to the one of the train word\n",
    "    \n",
    "    => compute true negative and false negative\n",
    "    \n",
    "c)  compute: \n",
    "\n",
    "    Precision = True Positives / (True Positives + False Positives)\n",
    "    \n",
    "    Recall = True Positives / (True Positives + False Negatives)\n",
    "    \n",
    "    => add [precision,recall] to a global list with the values obtained for each k\n",
    "    \n",
    "    => then compute the mean for precision and recall\n",
    "\n",
    "### The implementation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the results with most similar one on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(dictionary):\n",
    "    result = {k: v for k, v in sorted(dictionary.items(), key=lambda item: item[1])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the cost_results\n",
    "for word in cost_results.keys():\n",
    "    cost_results[word] = sort_dict(cost_results[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_results['300-02-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the results into either positive (1) or negative (0) with ground truth and keep words that appear at least once into train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binarized_results = {}\n",
    "# final_results is the dictionary that contains only meaningful results\n",
    "final_results = {}\n",
    "for val_word in cost_results.keys():\n",
    "    binarized_list = []\n",
    "    val_transcription = transcription[val_word]\n",
    "    #keep only the words that appear both in validation and training set\n",
    "    if val_transcription in keywords_list:\n",
    "        final_results[val_word] = cost_results[val_word]\n",
    "        for train_word in cost_results[val_word].keys():\n",
    "            train_transcription = transcription[train_word]\n",
    "            if val_transcription == train_transcription:\n",
    "                binarized_list.append(1)\n",
    "            else:\n",
    "                binarized_list.append(0)\n",
    "        binarized_results[val_word] = binarized_list\n",
    "print('Results are now filtered and binarised into positive or negative retrieval.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute precision and recall for every word and for every threshold k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes some time to compute, don't worry (1-2 min max)\n",
    "precision_recall_data = {}\n",
    "for word in final_results.keys():\n",
    "    lr_precision = []\n",
    "    lr_recall = []\n",
    "    for k in range(0, len(final_results[word])):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for r in range(0,k):\n",
    "            if binarized_results[word][r] == 1:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        for r in range(k, len(final_results[word])):\n",
    "            if binarized_results[word][r] == 1:\n",
    "                fn = fn + 1\n",
    "            else:\n",
    "                tn = tn + 1\n",
    "        if (tp+fp) == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "        if (tp+fn) == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "        lr_precision.append(precision)\n",
    "        lr_recall.append(recall)\n",
    "    precision_recall_data[word] = [lr_precision, lr_recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the precision-recall curve for every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in final_results.keys():\n",
    "    lr_precision, lr_recall = precision_recall_data[word][0], precision_recall_data[word][1]\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    print(f'Word : {word} ({transcription[word]})')\n",
    "    print('Logistic: AP=%.3f' % lr_auc)\n",
    "\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the final precision-recall curve (average of all the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mean precision and mean recall\n",
    "lr_mean_precision = []\n",
    "lr_mean_recall = []\n",
    "for i in range(0, len(final_results[word])):\n",
    "    lr_mean_precision.append(0)\n",
    "    lr_mean_recall.append(0)\n",
    "# compute the mean precision and recall   \n",
    "for word in final_results.keys():\n",
    "    lr_precision, lr_recall = precision_recall_data[word][0], precision_recall_data[word][1]\n",
    "    lr_mean_precision = [x + y for x, y in zip(lr_mean_precision, lr_precision)]\n",
    "    lr_mean_recall = [x + y for x, y in zip(lr_mean_recall, lr_recall)]\n",
    "    \n",
    "lr_mean_precision= [idx/len(final_results) for idx in lr_mean_precision]\n",
    "lr_mean_recall= [idx/len(final_results) for idx in lr_mean_recall]\n",
    "\n",
    "# plot the results\n",
    "lr_mean_auc = auc(lr_mean_recall, lr_mean_precision)\n",
    "print('Logistic: AP=%.3f' % lr_mean_auc)\n",
    "plt.plot(lr_mean_recall, lr_mean_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
